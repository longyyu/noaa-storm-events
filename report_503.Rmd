---
title: |
  | Temporal projection of natural atmospheric events 
  | in the United States Regions
author: "Haejeong Choi, Yanyu Long, Seung Ho Woo"
date: "April 2021"
output: 
  bookdown::pdf_document2:
    keep_tex: true
    number_sections: true
    toc: false
header-includes:
- \usepackage{float}
- \usepackage{titling}
- \setlength{\droptitle}{-5em}
- \usepackage{makecell}
- \usepackage{booktabs}
documentclass: article
geometry: left=0.85in,right=0.85in,top=0.85in,bottom=0.75in
fontsize: 11pt
urlcolor: blue
bibliography: references.bib
csl: chicago-author-date-16th-edition.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  include = TRUE,
  comment='',
  fig.align = "center",
  fig.pos = "H"
)
options(knitr.kable.NA = '') 
library(tidyverse)
fmt_float = function(float_val) {
  sprintf("%4.3f", float_val)
}
# rm(list = ls()); rmarkdown::render("report_503.Rmd")
```

```{=latex}
\newcommand\norm{\mathrm{N}}
```

# Introduction

Extreme weather events such as hurricanes, floods, and droughts can often lead to detrimental effects on agricultural production, severe infrastructure and property damage, and loss of life. The U.S. has seen a rapid increase in weather and climate disasters in recent years. In 2020, there were 22 "billion-dollar" extreme weather events (events where losses exceed $1 billion) in the U.S., hitting a record high since 1980 [@noaa-billion]. The most recent billion-dollar event was the Winter Storm Uri in February 2021, which caused at least 136 deaths and damage of over $195 billion [@enwiki:1019729582].  

Climate change is expected to increase the frequency and intensity of these events. 
For example, in dry climates, higher temperature can lead to an increase in surface evaporation, rapid drying of soils, and the occurrence of severe droughts [@briffa2009wet; @cai2009rising]. In wet climates, warmer weather can lead to abundant atmospheric water vapors [@willett2008recent], and the increase in water vapors is found to be a primary cause for the increase in extreme precipitation events [@kunkel2013monitoring]. 

In this project, we study the relationship between weather conditions and the occurrences of extreme weather events in the U.S. Our goal is to identify the most relevant indicators and the best-performing classification algorithms and build an alarm system of weather and climate disasters based on monthly weather statistics. 

# Data

## Data Preprocessing

#### The Storm Event Database {-}
For the outcome variable, this study uses the Storm Event Database maintained by the National Weather Service  [@noaa-storm]. This dataset documents the occurrence of extreme weather events such as thunderstorm wind, flash flood, drought, etc. at the county level from 1950 to 2020. We will compute the number of episodes observed in each county in each month,^[Each event in the NOAA database has an `episode_id` and a `event_id`. An episode can correspond to multiple related events. This study uses episodes as the unit of extreme weather events.] and transform it into a boolean variable `event` which equals one when there is at least one extreme weather event, and zero otherwise.

#### The historical weather dataset {-}
For the predictors, it is difficult to obtain weather data aggregated at the county level, so we will instead look at a city level historical weather dataset from the OpenWeatherMap API [@kaggle-weather] and use the city-level weather data to approximate the county-level conditions. This dataset contains hourly observations of temperature, humidity, pressure, wind speed, and wind direction in 27 U.S. cities (see Table \@ref(tab:us-cities)) from 2012 to 2017. To reduce the dimensionality and impute the missing values, monthly summary statistics are extracted from the hourly weather data by first computing daily average values of the observed variables, then computing the monthly average of the daily records.  

#### Merging the two datasets {-}
Due to the limited coverage of the weather data, we only look at the time frame of November 2012 - December 2017, and focus on the 27 counties we have weather records for. To match the cities in the weather dataset to the counties in the wind storm dataset, we use the city names and locations (latitude and longitude) information from the United States Cities Database [@us-cities]. We specifically match the extreme weather event records to the weather data in the previous month, allowing the model to forecast future events with observed weather measurements. 


```{r}
# the US cities dataset - helps match cities to states and counties
us_cities = read.csv("data/uscities.csv", stringsAsFactors = FALSE) %>%
  select(state_id, state = state_name, 
         county_fips, county = county_name, city, lat, lng) %>% 
  mutate(city = ifelse(city == "St. Louis", "Saint Louis", city))

# join the 27 cities weather dataset and the US cities dataset by city name, longitude and latitude 
city = read.csv(sprintf("data/weather/%s", "city_attributes.csv"), 
                stringsAsFactors = FALSE) %>% 
  filter(Country == "United States") %>% 
  left_join(us_cities, by = c("City" = "city")) %>%
  # when there are multiple cities with the same names, keep the record with the minimum distance
  mutate(distance = sqrt((Latitude - lat)^2 + (Longitude - lng)^2)) %>% 
  group_by(City) %>% filter(distance == min(distance)) %>% ungroup() %>%
  select(state, county, county_fips, city = City, lat = Latitude, long = Longitude) 

load(file = "cache/windstorm_num_episodes_by_state_2012_2017.RData")
load(file = "cache/usa_map_data.RData")
```

#### The final dataset {-}
The final dataset we obtain is relatively balanced, with 948 negative samples and 726 positive ones. We randomly draw 25% of the observations to be used as the test set. As the predictors are not on the same scale, we scale each feature variable individually so that they are all in the range of [0,1] before entering models.  

```{r}
tibble(
  Variable = c("event", "meantemp_avg", "difftemp_avg", "humidity_avg", 
               "pressure_avg", "wind_speed_avg", "wind_direction_avg", "state"),
  Description = c(
    # "number of extreme weather episodes observed in a county in a given year-month",
    "outcome variable, equals 1 when num_episodes>0, and 0 otherwise", 
    "monthly average of daily mean temperature (Kelvins)",
    "monthly average of diurnal temperature variation (max(T)-min(T), Kelvins)",
    "monthly average of daily mean humidity (%)",
    "monthly average of daily mean atmospheric pressure (hPa)",
    "monthly average of daily mean wind speed (meter/sec)",
    "monthly average of daily mean wind direction (degrees, meteorological)",
    "names of the states that the counties are in (categorical)"
  )
) %>% 
  knitr::kable(format = "latex", booktabs = TRUE, 
               caption = "Descriptions of variables used in the modeling process") %>%
  kableExtra::row_spec(row = 0, bold = TRUE, align = 'c') %>%
  kableExtra::add_footnote(label = "Observations are uniquely identified by county and year-month.") %>%
  kableExtra::kable_styling(latex_options = c("HOLD_position", "scale_down"))
```

## Exploratory Data Analysis  

To look into the relationship between the predictors and outcome variable, we will perform some descriptive data analysis. 
From the pairwise scatterplots between the predictors (Figure \@ref(fig:ggpairs)), we do not observe highly correlated features, except that the average diurnal temperature variation (`difftemp`) and the humidity have a slightly worrying but still acceptable correlation coefficient of 0.746. 
From the boxplots of the weather measurements against the outcome variable, we can see that higher temperature is positively associated with the probability of having extreme weather events. It seems that our research question will likely require a complicated model that allows for nonlinear decision boundaries and/or interactions between predictors, so we would expect algorithms like nonlinear Support Vector Machines (SVM) and tree-based ensemble methods to work better on this dataset.


From Figure \@ref(fig:heatmap), we can see that the probability of encountering severe weather events are different in each state. Texas had a total of 3,750 episodes from 2012 to 2017 and ranked first among all states. Texas experiences multiple types of weather conditions - the western one-third of the state suffers from cold winters and low humidity, while the eastern two-thirds experiences sub-tropical weather. The state sustains frequent thunderstorms which can lead to damaging hails and flash flooding, and all of Texas is susceptible to drought induced by the intense summer heat and the lack of rain [@molly2021what]. 
We will introduce the state as a predictor to account for the geographical factors affecting the probability of having extreme weather events other than weather conditions. 


```{r, eval = FALSE}
load("data/windstorm_weather_prev_month.RData")
data_merged = data_merged %>% select(-ends_with(c("sd"))) %>%
  mutate(event = ifelse(num_episodes > 0, 1, 0))

data_merged %>% select(event, ends_with("avg")) %>% 
  rename_with(.fn = function(x) str_replace(x, "_avg", "")) %>%
  mutate(event = as.factor(event)) %>%
  GGally::ggpairs(axisLabels = 'none',
    lower = list(
      continuous = GGally::wrap(
        "points", alpha = .6, size = .9,
        color = ifelse(data_merged$event == 1, "#A50F15", "grey70")),
      combo = GGally::wrap("box_no_facet", width = .6)
    ),
    diag = list(discrete = GGally::wrap("barDiag", width = .6))
  ) +
  theme_bw() + theme(strip.text = element_text(size = 9))
ggsave("cache/eda_pairwise_scatter.png", width = 6.5, height = 5.5, dpi = 240)
```

```{r ggpairs, fig.height = 6, fig.cap = "Density plots, pairwise scatterplots, and boxplots of the predictors (Red points: event = 1, grey points: event = 0)"}
knitr::include_graphics("cache/eda_pairwise_scatter.png")
```


```{r, fig.width = 5, fig.height = 2.3, fig.cap = "Cumulative number of extreme weather events (episodes) in each state, 01/2012 - 12/2017", eval = FALSE}
mapdata %>% 
  mutate(state = toupper(id)) %>%
  left_join(ws_state, by = c("state" = "state")) %>%
  arrange(order) %>%
  ggplot() +
    theme_bw() + 
    geom_polygon(aes(long, lat, group = group, fill = num_episodes), 
                 colour = "grey", size = 0.1) +
    scale_fill_gradient(
      name = "# of episodes", low = "#FFF5F0", high = "#A50F15",
      na.value = "#F2F2F2"
    ) +
    scale_x_continuous(limits = c(-125, -68)) +
    coord_map("polyconic") +
    labs(x = "", y = "") +
    theme(legend.title = element_text(size = 10),
          legend.box.margin = margin(0, 0, -0.5, 0, unit = "cm"),
          legend.key.width = unit(.5, "cm"),
          plot.margin=grid::unit(c(0, 0, 0, 0), "mm"))
ggsave("cache/eda_heatmap.png", width = 5, height = 2.3, dpi = 220)
```

```{r heatmap, fig.height = 2.3, fig.cap = "Cumulative number of extreme weather events (episodes) in each state, 01/2012 - 12/2017"}
knitr::include_graphics("cache/eda_heatmap.png")
```

# Methodology

## Research Approach
This project aims to identify the best-performing classification algorithms and the most important indicators that help forecast future extreme weather events. Therefore, we would want to compare multiple classification methods that excel at either interpretability or predictive power. We will consider the following algorithms: Naive Bayes, Nearest Neighbors, Decision Tree, AdaBoost, Random Forest, Support Vector Machine (SVM) with a Radial Basis Function (RBF) kernel,  Neural Networks, and Gaussian Process Classifier. For each algorithm, we tune the parameters using cross validation, and evaluate the performance on the test set using the following metrics: (1) $\text{Accuracy} = (TP+TN)/(TP+FP+FN+TN)$; (2) $\text{Balanced Error Rate (BER)} =  0.5(FP/(TN+FP) + FN/(FN+TP))$; and (3) the area under the receiver operating characteristic curve (AUC). The accuracy and BER will evaluate the predicted labels, while the AUC can evaluate the predicted probabilities. 

## Preliminary Screening
Since there are too many available classification methods to be considered, and we have inferred from the EDA that certain types of classifiers might work better on this dataset than others (i.e., those with nonlinear decision boundaries or allow variable interactions), we would not want to tune parameters for all of them. 
Instead, we will perform a preliminary screening by evaluating the model performance using the untuned parameters, and only tune the parameters for the set of algorithms that look most promising. 
As the Table \@ref(tab:all-method-metrics) show, the Gaussian Process Classifier, RBF-SVM, and Random Forest have much better accuracy, BER, and AUC than other competing methods. Thus, we will focus on these three classification methods to tune the parameters and evaluate their performance.  


```{r all-method-metrics}
tribble(
  ~Classifier, ~Accuracy, ~BER, ~AUC,
  "Gaussian Process Classifier",  "68.11%", "0.3277", "0.7078",
  "RBF-Support Vector Machine",   "66.43%", "0.3425", "0.7102",
  "Random Forest",   "65.95%", "0.3281", "0.7230",
  "Neural Network", "64.75%", "0.3828", "0.6405",
  "Nearest Neighbors", "64.51%", "0.3658", "0.6808",
  "Naive Bayes", "63.55%", "0.4113", "0.6266",
  "AdaBoost", "63.31%", "0.3788", "0.6905",
  "Decision Tree", "62.35%", "0.3619", "0.6489"
  # "Linear Support Vector Machine", "60.43%", "", ""
) %>% 
  knitr::kable(format = "latex", booktabs = TRUE, align = "lc",
               caption = "Accuracy, BER, and AUC of different classification algorithms") %>%
  kableExtra::row_spec(row = 0, bold = TRUE, align = 'c') %>%
  kableExtra::kable_styling(latex_options = c("HOLD_position"))
```

## Hyperparameter optimization

#### Random Forest {-}

The Random Forest is an ensemble algorithm that builds multiple base classifiers (in our case, decision trees) and combines them together for more stable predictions. It enables uncorrelated models to operate together and head in the correct direction since the majority of them would make the right decisions even if some are wrong. 

#### SVM {-}

Using the Support Vector Machines, the most important parameter to tune is the kernel. Some of the commonly used nonlinear kernels are polynomial, radial basis, and Sigmoid function kernels. We would expect a polynomial or radial basis kernel to perform better as they allow more flexible decision boundaries. 

#### Gaussian Process Classifier {-}

The Gaussian Process Classifier is based on a Bayesian methodology. It assumes some prior distribution on the underlying probability densities that guarantee some smoothness properties. As a probabilistic method, it allows us to compute empirical confidence intervals and decide if one should refit the prediction in some region of interest. The final classification is then determined as the one that provides a good fit for the observed data, while at the same time guaranteeing certain level of smoothness. To train a GP classifier, we need to specify a kernel function which describes the covariance of the prior distribution. 



# Results

We use the Python Machine Learning Library `scikit-learn` to implement the models and tune the hyperparameters [@scikit-learn]. 

#### Random Forest {-}

With a randomized grid search, we found the the following setting performs the best: training 800 base classifiers with a maximum depth of 40, considering $\sqrt{p}$ features in each split where $p$ is the total number of features, splitting an internal node only when there are at least 15 samples in the current node and at least 2 samples in each of the left and right branches, and using the Gini index as the impurity criterion.  

Using the tuned Random Forest classifier, the accuracy on our training data set and test set are 91.87% and 69.06%, respectively. As we can see from the receiver operating characteristic curve (ROC) (Figure \@ref(fig:roc-all)) and the confusion matrix (Table \@ref(tab:conf-mat-all)), the AUC reaches 0.7230 and the BER is 0.3281. The Random Forest classifier seems to work well in terms of accuracy and AUC, but it makes a lot of type II errors and thus cannot identify weather conditions that will lead to severe weather events, which is against the goal of our study. 

```{r roc-conf-mat-rf, fig.show = "hold", out.width = "48%", eval = FALSE}
knitr::include_graphics("cache/results_roc_rf.png")
knitr::include_graphics("cache/results_conf_mat_rf.png")
```

The Random Forest classifier has better interpretability than the other two models, as it allows us to compare the importance of features (Figure \@ref(fig:feature-imp)). [TODO: add more analysis]

```{r feature-imp, out.width = "60%", fig.cap = "Relative influence plot of the features"}
knitr::include_graphics("cache/results_feature_importance_rf.png")
```

#### SVM {-}

The best performance is given by an RBF kernel SVM with $\gamma=1$ and $C=5$. The parameter $\gamma$ defines how far the influence of a single training example reaches, and $C$ is the coefficient for the penalty term, determines how much error is bearable and controls the bias-variance trade-off. We have relatively low $\gamma$ and $C$ values, which means that we give less weight to the training data and our model will have a simpler decision function with a larger margin.  

The tuned SVM model gives a training-set accuracy of 73.92%, a test accuracy of 66.19%, an AUC of 0.7102 and a BER of 0.3425. It is less satisfactory than the Random Forest in terms of accuracy, AUC and BER, but it also makes less type II errors. 


#### Gaussian Process Classifier {-}

Of the available kernel functions, we find that an RBF kernel works best for this dataset. Since the Gaussian Process Classifier is a Bayesian method, the rest of the hyperparameters are optimized during fitting. 

The tuned GP classifier gives a training-set accuracy of 73.68%, test accuracy of 68.11%, AUC of 0.7078 and BER of 0.3277. We choose this model as the optimal classifier as it has fairly good accuracy, AUC, and BER and it classified both true 0 and 1 samples well. 


```{r roc-all, out.width = "60%", fig.cap = "Receiver operating characteristic curves of the three algorithms with tuned hyperparameters"}
knitr::include_graphics("cache/results_roc_3models.png")
```

```{r conf-mat-all}
tribble(
  ~Model, ~TN, ~FP, ~FN, ~TP,
  "Random Forest", 192, 60, 69, 96,
  "RBF-SVM", 171, 81, 60, 105,
  "Gaussian Process", 180, 72, 61, 104
) %>% 
  knitr::kable(format = "latex", booktabs = TRUE, 
               caption = "Confusion matrix of the three algorithms with tuned hyperparameters") %>%
  kableExtra::row_spec(row = 0, bold = TRUE, align = 'c') %>%
  kableExtra::kable_styling(latex_options = c("HOLD_position"))
```

# Conclusions

This project studies the relationship between extreme weather events in the U.S. and historical weather data. We tried different classification algorithms to compare their performance. 
When it comes to predictive power, the Gaussian Process Classifier has the best BER, while the xxx [TODO: update] has the best AUC value. 
With the Random Forest classifier, we are able to get a summary of the variable importance. Variable xxx [TODO: update] is the most helpful indicator for forecasting extreme weather events, followed by xxx [TODO: update]. 

# Future Work

When constructing the outcome variable, this study aggregates all types of extreme weather events, so we are only able to discuss how weather measurements contribute to the probability that any type of events will happen. This may not be an ideal practice as the relationship between the predictors and the outcome might vary if we look at different types of events. To take this study further, we can divide the events into subcategories or even look at individual event types for better predictive power and more meaningful parameter interpretations. 

When it comes to the predictors, this study uses the city-level data as an approximation for counties' weather conditions due to the difficulty of gathering county-level weather data. However, the cities where weather data are available are not guaranteed to represent the counties well. In the future, we can select multiple land surface stations from each county and construct a more representative county-level weather dataset, which might generate more accurate predictions. 


# References {-}

<div id="refs"></div>

# Appendix {-} 

```{r us-cities, echo = FALSE}
city1 = city %>% select(city, state, county) %>% slice(1:14)
city2 = city %>% select(city, state, county) %>% slice(15:27) %>% add_row()
bind_cols(city1, city2) %>%
  knitr::kable(format = "latex", booktabs = TRUE, 
               caption = "The 27 U.S. cities where weather data are available",
               col.names = rep(c("city", "state", "county"), 2)) %>%
  kableExtra::row_spec(row = 0, bold = TRUE, align = 'c') %>%
  kableExtra::column_spec (3, border_right = TRUE) %>%
  kableExtra::column_spec (c(1, 4), bold = TRUE) %>%
  kableExtra::kable_styling(latex_option = "HOLD_position")
```
