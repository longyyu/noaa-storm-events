---
title: "Temporal projection of natural atmospheric events in the United States Regions"
author: "Haejeong Choi, Yanyu Long, Seung Ho Woo"
date: "Apr 18, 2021"
output: 
  beamer_presentation:
    theme: "AnnArbor"
header-includes:
  - \AtBeginDocument{\title[STATS503 Project]{Temporal projection of natural atmospheric events in the United States Regions}}
  - \AtBeginDocument{\author[Group 7]{Haejeong Choi, Yanyu Long, Seung Ho Woo}}
  - \usepackage{booktabs}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  comment = '',
  message = FALSE,
  warning = FALSE,
  fig.align = 'center'
)
library(tidyverse)
# rm(list = ls()); rmarkdown::render("pre_503.Rmd")
```

## Introduction

### Motivation

- The U.S. experiences a variety of extreme weather events, including hurricanes, floods, blizzards, droughts, and so on, causing severe infrastructure damage and health impacts. 
- Climate change is expected to increase the frequency and intensity of these events.

```{r, eval = FALSE, echo = FALSE}
df = read.csv("data/StormEvents_details-ftp_v1.0_d2012_c20200317.csv",
              stringsAsFactors = FALSE) %>% 
  select("BEGIN_YEARMONTH", "EVENT_ID", "EPISODE_ID", "EVENT_TYPE")

df %>% 
  group_by(EVENT_TYPE) %>%
  summarise(n = n()) %>% arrange(-n) %>%
  filter(EVENT_TYPE == "Flood")
```

### Objectives

- Study the relation between weather conditions and the occurrences of extreme weather events in the U.S.
- Identify the most relevant indicators and the best-performing classification algorithms to predict natural disasters based on daily weather statistics. 


## Dataset 1: Storm Event Data

- Data source: [National Centers for Environmental Information (NOAA)](https://www1.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/)
- Documents extreme weather events such as drought, flood, wildfire, hurricane, etc. at the county level from 1950 to 2020.
    - We only look at 2012 - 2017 records to match the weather dataset.
- We compute the number of episodes (a series of related events) observed in each county in each month from 2012 to 2017. 

```{r, echo = FALSE, eval = TRUE}
load(file = "data/windstorm_num_episodes_by_county_ym_2012_2017.RData")
windstorm %>% head() %>% knitr::kable(format = "latex", booktabs = TRUE) %>%
  kableExtra::kable_styling(position = "center")
```

---

- Cumulative number of extreme weather events (episodes) in each state, 01/2012 - 12/2017

```{r, fig.width = 4.5, fig.height = 3}
load(file = "cache/windstorm_num_episodes_by_state_2012_2017.RData")
load(file = "cache/usa_map_data.RData")
centroids_20 = centroids %>% 
  mutate(state_name = toupper(state_name)) %>%
  left_join(ws_state, by = c("state_name" = "state")) %>% 
  arrange(-num_episodes) %>% head(20)

mapdata %>% 
  mutate(state = toupper(id)) %>%
  left_join(ws_state, by = c("state" = "state")) %>%
  arrange(order) %>%
  ggplot() +
    theme_bw() + 
    geom_polygon(aes(long, lat, group = group, fill = num_episodes), 
                 colour = "grey", size = 0.1) +
    scale_fill_gradient(
      name = "# of episodes", low = "#FFF5F0", high = "#A50F15",
      na.value = "#F2F2F2"
    ) +
    scale_x_continuous(limits = c(-125, -68)) +
    geom_text(data = centroids_20, aes(x, y, label = state_abbr), size = 1.3) +
    coord_map("polyconic") +
    labs(x = "", y = "") +
    theme(legend.title = element_text(size = 10),
          legend.key.width = unit(.5, "cm"))
```


## Dataset 2: Hourly weather dataset

* Historical hourly weather data in 27 U.S. cities from 2012 to 2017.

```{r}
# the US cities dataset - helps match cities to states and counties
us_cities = read.csv("data/uscities.csv", stringsAsFactors = FALSE) %>%
  select(state_id, state = state_name, 
         county_fips, county = county_name, city, lat, lng) %>% 
  mutate(city = ifelse(city == "St. Louis", "Saint Louis", city))

# join the 27 cities weather dataset and the US cities dataset by city name, longitude and latitude 
city = read.csv(sprintf("data/weather/%s", "city_attributes.csv"), 
                stringsAsFactors = FALSE) %>% 
  filter(Country == "United States") %>% 
  left_join(us_cities, by = c("City" = "city")) %>%
  # when there are multiple cities with the same names, keep the record with the minimum distance
  mutate(distance = sqrt((Latitude - lat)^2 + (Longitude - lng)^2)) %>% 
  group_by(City) %>% filter(distance == min(distance)) %>% ungroup() %>%
  select(state, county, county_fips, city = City, lat = Latitude, long = Longitude) 
```

```{r, fig.width = 3.5, fig.height = 2.3}
map_cache = "cache/usa_map_data.RData"
if (!file.exists(map_cache)) {
  library(albersusa) # remotes::install_github("hrbrmstr/albersusa")
  mapdata = usa_composite()
  centroids = rgeos::gCentroid(usa_composite(), byid = TRUE) %>%
    as.data.frame() %>%
    mutate(state_abbr = mapdata@data$iso_3166_2,
           state_name = mapdata@data$name)
  mapdata = mapdata %>% fortify(us, region = "name")
  save(centroids, mapdata, file = map_cache)
}
load(file = map_cache)

ggplot() +
  theme_bw() + 
  geom_polygon(data = mapdata, aes(long, lat, group = group),
               colour = "grey", fill = "#f7e1c8", size = .2) +
  scale_x_continuous(limits = c(-125, -70)) +
  coord_map("polyconic") +
  geom_point(data = city, aes(long, lat), color = "darkred", size = 1.2, alpha = .6) +
  # geom_text(data = centroids, aes(x, y, label = state_abbr), size = 1.3, color = 'grey70') +
  labs(x = "", y = "")
```

## Dataset 2: Hourly weather dataset

* Contains hourly observations for humidity, pressure, temperature, wind direction, and wind speed (all numeric).
* Convert to monthly observations to match with the windstorm dataset and deal with missing values.
    - hourly observations $\rightarrow$ daily mean values $\rightarrow$ monthly average and standard deviation
    - 12 numeric weather measurements

## Final dataset

- Convert number of episodes to a boolean type - a classification problem.
- Matches windstorm records to the weather data in the previous month - a forecasting problem.
- This is a relatively balanced dataset (negative:positive = 948:726)

```{r, echo = FALSE, eval = FALSE}
load("data/windstorm_weather_prev_month.RData")
data_merged = data_merged %>% mutate(event = ifelse(num_episodes > 0, 1, 0))
table(data_merged$event)
```

- Train-test split: 75/25 (random)

## Model Selection

Algorithm | Error Rate | Balanced Error Rate | AUC
 -------- | ---------- | ------------ | ----------
GAM | 
Random Forest |
SVM |

